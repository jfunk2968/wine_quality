---
title: "Wine Quality"
output: html_notebook
---

## Machine learning with Caret

Load Wine Quality data

```{r}
white <- read.csv('~/Desktop/wine_quality/winequality-white.csv', sep=";")
print(str(white))
```

```{r}
library(ggplot2)

white$q <- as.factor(white$quality)

ggplot(data=white, aes(x=q)) +
  geom_bar() +
  labs(x = "Wine Quality")
```


Load Caret package and partition data into Test and Train

```{r}
library(caret)
set.seed(507483)

trainIndex <- createDataPartition(white$quality, p = .6, times = 1, list = F)
white$class <- as.factor(ifelse(white$quality > 7, 'good', 'bad'))

wTrain <- white[trainIndex, !(names(white) %in% c('quality','q'))]
wTest <- white[-trainIndex, !(names(white) %in% c('quality','q'))]

str(wTrain)
```

<!-- Set up backend to run in paralell - DONT USE WITH XGBOOST -->
<!-- ```{r} -->
<!-- library(doParallel) -->
<!-- registerDoParallel(4) -->
<!-- getDoParWorkers() -->

<!-- registerDoSEQ() -->
<!-- ``` -->


Fit an xgboost model with grid parameter search
```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5,
                           summaryFunction = twoClassSummary,
                           classProbs = T)

grid <- expand.grid(nrounds = c(100, 200),
                    eta = c(.01, .1),
                    max_depth = c(3, 5),
                    gamma=c(0), 
                    colsample_bytree=c(1), 
                    min_child_weight=c(1), 
                    subsample=c(.5, .75))
  
gbmFit1 <- train(class ~ .,
                 data = wTrain,
                 method = 'xgbTree',
                 metric = 'ROC',
                 trControl = fitControl,
                 tuneGrid = grid,
                 verbose = 1,
                 allowParallel = F,
                 nthread = 4)

gbmFit1
```


It appears that a lower with additional trees still has opportunity to impove ...
```{r}
plot(gbmFit1)
```


Run a random parameter search
```{r}
fitControlR <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5,
                           summaryFunction = twoClassSummary,
                           classProbs = T,
                           search = "random")
  
gbmFitR <- train(class ~ .,
                 data = wTrain,
                 method = 'xgbTree',
                 metric = 'ROC',
                 trControl = fitControlR,
                 tuneLength = 30,
                 verbose = 1,
                 allowParallel = F,
                 nthread = 4)

gbmFitR

plot(gbmFitR)
```



Use bayesian optimization to tune parameters, with random search as starting point
```{r}
ctrl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5,
                           summaryFunction = twoClassSummary,
                           classProbs = T )

xgb_fit_bayes <- function(nroundsIn, etaIn, max_depthIn, gammaIn, colsample_bytreeIn, min_child_weightIn, subsampleIn) {
  txt <- capture.output(
    mod <- train(class ~ .,
                 data = wTrain,
                 method = 'xgbTree',
                 metric = 'ROC',
                 trControl = ctrl,
                 tuneGrid = data.frame(nrounds = nroundsIn, 
                                       eta = etaIn, 
                                       max_depth = max_depthIn, 
                                       gamma = gammaIn, 
                                       colsample_bytree = colsample_bytreeIn, 
                                       min_child_weight = min_child_weightIn, 
                                       subsample = subsampleIn)))

  list(Score = getTrainPerf(mod)[, "TrainMOC"], Pred = 0)
}

## Define the bounds of the search. 
lower_bounds <- c(nroundsIn = 50, 
                  etaIn = .0001, 
                  max_depthIn = 1, 
                  gammaIn = 0,
                  colsample_bytreeIn = 1, 
                  min_child_weightIn = 1,
                  subsampleIn = .5)

upper_bounds <- c(nroundsIn = 1000, 
                  etaIn = .2, 
                  max_depthIn = 5, 
                  gammaIn = 0,
                  colsample_bytreeIn = 1, 
                  min_child_weightIn = 1,
                  subsampleIn = .9)

bounds <- list(logC = c(lower_bounds[1], upper_bounds[1]),
              logSigma = c(lower_bounds[2], upper_bounds[2]))


## Create a grid of values as the input into the BO code
initial_grid <- rand_search$results[, c("C", "sigma", "RMSE")]
initial_grid$C <- log(initial_grid$C)
initial_grid$sigma <- log(initial_grid$sigma)
initial_grid$RMSE <- -initial_grid$RMSE
names(initial_grid) <- c("logC", "logSigma", "Value")

## Run the optimization with the initial grid and do
## 30 iterations. We will choose new parameter values
## using the upper confidence bound using 1 std. dev. 
library(rBayesianOptimization)
 
set.seed(8606)
ba_search <- BayesianOptimization(xgb_fit_bayes,
                                 bounds = bounds,
                                 init_grid_dt = initial_grid, 
                                 init_points = 0, 
                                 n_iter = 30,
                                 acq = "ucb", 
                                 kappa = 1, 
                                 eps = 0.0,
                                 verbose = TRUE)

```






